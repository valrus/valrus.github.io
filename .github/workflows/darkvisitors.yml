name: Fetch and Commit robots.txt
# https://darkvisitors.com/docs/robots-txt

on:
  schedule:
    - cron: "55 2 * * *"
  workflow_dispatch:  # Allow manual triggers

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure Git
      run: |
        git config user.name "github-actions"
        git config user.email "github-actions@github.com"

    - name: Fetch robots.txt
      env:
        ACCESS_TOKEN: ${{ secrets.DARK_VISITORS_ACCESS_TOKEN }}
      run: >
        curl -X POST https://api.darkvisitors.com/robots-txts -H 'Authorization: Bearer ${ACCESS_TOKEN}' -H 'Content-Type: application/json' -d '{"agent_types": ["AI Data Scraper", "Undocumented AI Agent"], "disallow": "/"}' -o robots.txt

    - name: Commit and push robots.txt
      run: |
        git add robots.txt
        git commit -m "Update robots.txt daily from darkvisitors API"
        git push
